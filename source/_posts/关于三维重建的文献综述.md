---
title: 关于三维重建的文献综述
date: 2019-01-04 07:58:17
tags: 
    - 3D model reconstruction
    - 文献综述
categories: 论文综述
---

# 目录

[arXiv检索](https://arxiv.org/)

1. 1812.10558 通过视频素材实现从2d到3d的面部重建来完成测谎
2. 1812.01742 单一视角的三维重建，使用对抗训练（非人
3. 1812.05583 基于学习的ICP（迭代最近点算法）重构场景（非人
4. 1812.07603 通过视频素材的面部模型学习
5. 1812.05806 自我监督的引导方法，单图片的三维人脸重建
6. 1812.02822 学习生成模型的隐藏区域（非人
7. 1901.00049 基于轮廓的衣着人物（全身

**A类**

* 通过直接体积cnn回归从单图重建大范围三维人脸（源码lua+py
* 使用图到图转换的无限制面部重建（源码lua

**老师推荐**

* 使用affinity field的实时多人二维姿态估计

# 综述

## 通用

**关于三维重建**  
单个图像进行三维重建的数据驱动方法：一是明确使用三维结构，二是使用其他信息推断三维结构  
2DImage-->encoder-->latent representation-->decoder-->3DObject  
不同方法区别在于对三维世界采取的**限制**：多视图一致性学习三维表示、利用关键点和轮廓注释、利用2.5D草图（法线，深度和轮廓）改善预测  

encoder-decoder的[含义](https://blog.csdn.net/chinabing/article/details/78763454)

**关于shape priors**
许多方法选择更好的捕捉多样的真实形状  
**non-deep方法**关注低维参数模型，使用CNN来学习2D渲染图像和3D形状的**共同嵌入空间**  
其他方法依赖**生成模型**去学习shape priors

## 博客链接

[3D人脸重建学习笔记CSDN](https://blog.csdn.net/yyyllla/article/details/84573393)  
[3D重建的学习笔记简书](https://www.jianshu.com/p/f33b3d440f7d)

## Learning Single-View 3D Reconstruction with Adversarial Training 1812.01742  

传统方法用多个角度的多张照片实现三维建模  
问题两个：一是需要大量的观察点；二是物体表面是*Lambertian*（非反射）albedos是非均匀的  
另一种三维重建的方式是利用物体外观和形状的知识从单视图二维图像生成（假设shape priors足够丰富）  
CAD库（computer-aided design）：<u>shapenet，pascal3d+，objectnet3d，pix3d</u>  

这些方法都从渲染的图像中回归三维形状：将二位图像转化成潜在表示的**编码器** 以及 重建三维表示的**解码器**  
为了学习shape priors深度学习算法需要大量的三维对象注释，自然图像中获取三维注释很有挑战，因此使用合成图像（三维模型渲染出的图像）  
CNN的<u>domain shift</u>问题，导致基于cnn的三维重建性能恶化  

这篇文章的方法：提高重建模型性能，为了实现获取三维物体标签，他们shape priors训练出的网络有个**重建损失值**，给这个值引入了两个限制  
一是受domain shift文献启示，强制让编码的二维特征不变，对应于他们所来自的domain。这样合成图像训练出的编码器在真实图像上表现更好  
二是将编码的二维特征限制在现实物体的多种形状之中，通过对抗训练定义这两个损失值  
总结：一个**模型**和**损失函数**，利用shape priors提高自然图像三维重建性能（两种方式使用对抗训练）  
reconstruction adversarial network(RAN)  
**只使用rgb图像信息**，和易于获取的自然图像。独立于编码器和解码器，并且可以使用到其中  
借鉴了domain confusion（作用是classification），为了让从合成图像里训练出来的模型在真实图像这边有更好的表现  

具体方法：todo

## 通过直接体积cnn回归从单图重建大范围三维人脸

目前三维人脸重建的方法多假定有多张面部图片可以使用，这使得重建面临方法上的挑战：在夸张的表情、不均匀光照上建立稠密对应关系  
这些方法需要复杂低效的管道构建模型，拟合模型。本文建议通过在由2D图像和3D面部模型或扫描组成的适当数据集上训练卷积神经网络
（CNN）来解决这些限制

## Extreme 3D Face Reconstruction: Seeing Through Occlusions 极端3D面部重建：遮挡透视（讲）

bumpingmapping概念的推动下，该文提出了一种分层方法。将全局形状与其中细节进行解耦。估计粗糙的3d面部形状为基础，然后将此基础与凹凸贴图表示的细节分开。
与本文相关的工作：
    reconstruction by example 这类方法用三维脸部形状去调整根据输入图片估计出的模型，降低了观看条件却损失了真实度与准确性  
    face shape from facial landmarks 这类方法稳定但是模型都差不多，没有细节，而且不清楚遮挡landmark的情况下表现会如何  
    SfS *Shape-From-shading* 根据光反射生成细节丰富的模型，但是受环境影响严重，需要满足其对环境的特殊要求。任何遮挡物都会生成到模型中  
    statistical representations 最著名的方法是3DMM，这篇文改进了这个方法直接根据图片强度信息用cnn回归3DMM的参数和面部细节  
    deep face shape estimation 深度网络一是直接用深度图重建，二是estimate 3D shapes with anemphasis on unconstrained photo 观察条件高度不变但是细节模糊  

**准备工作**  
矛盾：整体形状的高度正则化vs细节的弱正则化。解决方法：bump map representations which separate global shape from local details
    理解的正则化：使模型更有普适性，低正则化是让模型有更多细节、更有特点，反之是让模型更接近普适的规则（每个模型都有一只鼻子一张嘴两只眼睛）
给一张图片建立以下几个部分：基础形状——S，面部表情——E，6维度的自由视点——V。接下来是bump map捕捉中级特征（皱纹等非参数的），最后完成因遮挡丢失的细节。  
**添加细节**
基础形状使用3DMM，3DMM用了resnet的101层网络架构。表情部分由3DDFA提供，更新的有expnet。确定视点用了deep，facepostnet。  
中等程度细节：image to bump map，修复遮挡细节，基于软对称的模型完善。  
[LFW验证](http://vis-www.cs.umass.edu/lfw/)

PPT用：
目的：现有单图三维重建局限性很高，必须在正前方、距离近、无阻挡的视点，该文设计了一种用于在极端条件下提供细节丰富的面部三维重建模型的系统。极端条件包括，头部旋转以及遮挡
方法：简单讲步骤，关键的创新点，值得学习的点后边会细说。
总的来说：先创建面部整体的基础形状，与局部细节分开，在基础形状之上建立中等程度的面部特征。这样做可以保证极端条件下整体面部形状的稳定性。其他较新的方法往往用局部细节构建整体形状。
 构建基础形状s，构建面部表情e，构建视点v：*凹凸图可以分离整体形状和局部细节*
这仨东西分别是干什么用的：*基础形状使用3DMM，3DMM用了resnet的101层网络架构。表情部分由3DDFA提供，更新的有expnet。确定视点用了deep，facepostnet。*
image to bump map转换
凹凸图训练集：用深度编码-解码框架生成凹凸图
学习建立凹凸图：定义了自己的网络损失函数，可以在不牺牲高频细节的情况下抑制噪声
还原遮挡细节
给予范例的空洞填充方法
搜索参考集
混合细节
更复杂的修补
基于软对称的模型补全  

贡献：解决**对foundation的高度正则化** VS **对detail的低正则化** 两者的矛盾  

注：  
    bump map使用灰度值来提供高度信息，normal map使用xyz轴所对应的rgb信息
    [卷积与反卷积](https://github.com/vdumoulin/conv_arithmetic)

跑demo流程：
    NVIDIA-docker启动container，如果跑代码没有driver重新run一个，用readme里的run命令。
    之后会出现860m只支持cuda5.0的报错，需要[从源码编译pytorch](https://github.com/pytorch/pytorch#from-source)。首先docker里装anaconda
        wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2018.12-Linux-x86_64.sh
        应该不用在docker里装cuda和cudnn，直接安装pytorch的依赖然后安装pytorch应该就可以  
        在1080上不会出现上边的报错，完全按照README走就行。

[PPT](extreme_3d_face.pptx)

<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/extreme_3d_face.pptx' width=800 height=600 frameborder='1'></iframe>

## Learning to Estimate 3D Human Pose and Shape from a Single Color Image(讲) DOI:10.1109/CVPR.2018.00055

SCAPE:  shape  completion  and  animationof people
SMPL: A skinned multi-person linear model  
SMPL是一种参数化人体模型，与非参数化模型的区别在于，参数化的可以用函数映射的方式表达出来，或者说是可以解析的？非参数化则认为是通过实验记录到的模型，不存在解析表达式。  


Stacked Hourglass Networks
[资料一](https://blog.csdn.net/wangzi371312/article/details/81174452)
[资料二](https://blog.csdn.net/shenxiaolu1984/article/details/51428392)

[feature map](https://blog.csdn.net/dengheCSDN/article/details/77848246)
channel:
卷积核个数、特征图个数、通道个数关系

[PPT](Learning to Estimate 3D Human Pose and Shape from a Single Color Image.pptx)

<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/Learning to Estimate 3D Human Pose and Shape from a Single Color Image.pptx' width=800 height=600 frameborder='1'></iframe>

## O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis

还有adaptive o-cnn  
The main technical challenge of the O-CNN is to parallelize the O-CNN computations defined on the sparse octants so that they can be efficiently executed on the GPU  
We train this O-CNN model with 3D shape datasets and refine the O-CNN models with different back-ends for three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.

## Pixel2Mesh（讲）

### code

编译tensorflow math_functions.hpp找不到。需要软链接这个玩意  
ln -s /usr/local/cuda/include/crt/math_functions.hpp /usr/local/cuda/include/math_functions.hpp  

关于eigen和cuda[资料](https://blog.csdn.net/O1_1O/article/details/80066236)  
makefile怎么写。  
hdf5 HDF（Hierarchical Data Format）是一种设计用于存储和组织大量数据的文件格式

__CUDACC_VER__ is no longer supported.的报错看来要更新[eigen3](https://blog.csdn.net/luojie140/article/details/80159227)才能解决
github上新版eigen考到anaconda的eigen和support里就可以成功编译cuda了

图卷积神经网络[资料](http://tkipf.github.io/graph-convolutional-networks/)
图卷积神经网络[材料](https://cloud.tencent.com/developer/news/330322)
**所有的卷积都是在探讨如何对局部数据按照某一个操作聚合，不同的操作方式就对应于不同的卷积。**学习卷积核的过程其实是学习局部聚合参数的过程

[PPT](pixel2mesh.pptx)
<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/pixel2mesh.pptx' width=800 height=600 frameborder='1'></iframe>

## SMPL: A Skinned Multi-Person Linear Model(多篇基础，15年)

## Video Based Reconstruction of 3D People Models(讲，没用网络) DOI:10.1109/CVPR.2018.00875 (video2mesh)

第五页第一张图解决了人体非刚性的问题  
但是问题在于人必须转身后摆出相同的姿势 不允许姿势变化  
第二张图 可以随意动 不同时刻的深度图非张性的注册并融合到一个template上  
存在 phantom surface的问题 运动的快会四肢胳膊 两个头  
第三个 加了一个static的人体模型作为约束 运动速度可以更快  

第一个使用rgb相机 并且支持用户运动的重建方法  
主要思想 和visual hull相似 不同角度拍摄剪影进行重建  
visual hull的基本原理 几个角度拍摄 分割出前景得到silhouette  
然后从相机坐标到silhouette的每一个点可以做一条射线 形成的曲面成为silhouette cone  
用这些cone作为约束就可以重建出三维模型 可以类比为雕刻的过程  
去掉cone之外的部分 最终剩下的部分就是人体的形状  

标准的vh的问题是只能用于静态的物体 这篇的主要是讲怎么把vh用到动态的物体  
第八页 每帧姿势都不一样 要做的就是去除由于运动对cone造成的变化 称为unpose的过程  
用unpose的cone做三维重建  

使用的人体的三维表达：smpl 参数化模型 T是template的mean shape，Bs是体型变化造成的模型变化  
Bp是pose的变化带来的变化  
问题在于没有办法model衣服头发面部特征 基础上加了D offset用于表达smpl表达不了的信息  
  
四个步骤  
1 前景分割 获取silhouette cone， tracking获取人体模型的姿态  
2 利用pose信息做unpose操作 转到Tpose姿态下  
3 人体重建 包含衣服 头发 人脸的人体模型  
4 多视角图像生成人体贴图  

1 基于cnn的方法 2d drawn detection 图像分割的方法 前景分割生成silhouette  
优化第12页的能量函数 进行pose tracking //简单来说就是求最优的pose和shape的参数 和模型匹配到检测到的  
2d drawn detection和silhouette上//  
2 第一步得到的cone进行unpose 每一条射线进行unpose转到canonical pose下  
//两个数学表达式 射线的转换//  
任何一点vi 和 任何一条射线ri
3 利用unpose后的cone做三维重建 称为consensus shape，相比较SMPL/视频表现出的是可以对衣服进行重建  
过程可以通过优化一个能量公式实现  
Edata：模型上的点到unpose ray的距离  
三个正则项：lap保证局部光滑，body保证重建出的与smpl差距不大，symm保证左右对称  
4 有了几何信息后 生成appearance信息 生成texture map 第一步有每一帧的pose，精确的将模型覆盖到图像上  
通过//重投影获得贴图//  

用sfs（之前的文章有提到）可以提供更多细节，本文方法可以提高的地方
对**能量函数**的理解：构建能量函数就是我们用方程的最小值来描述我们想要达到的实际效果。[资料](https://blog.csdn.net/a6333230/article/details/80070586)

第一步最费时间 一帧一分钟 model和silhouette的匹配费时间  
穿裙子解决不了 改变不了smplmodel的拓扑结构 拉不过去  
基于cnn的分割已经接近于完美了 用的别人的方法 不是重点  
给纹理图上色：consensus shape 结合第一步的pose 精确匹配到每一帧的图像上 back projection  

{% pdf videobasedreconstructionof3dpeoplemodelsGAMES201850徐维鹏.pdf %}
[ppt](videobasedreconstructionof3dpeoplemodelsGAMES201850徐维鹏.pdf)

## Learning to Reconstruct People in Clothing from a Single RGB Camera（2019.4video2mesh延伸论文，同一实验室）octopus

安装dirt遇到的问题：https://github.com/pmh47/dirt/issues/23
已经尝试过cuda10.1/10.0/9.2 cudnn都是对应版本，tensorflow单独测试成功
更改gcc/g++版本：https://blog.csdn.net/u012925946/article/details/84584830

最终安装dirt解决方法是：  
ubuntu 18.04，cuda 8.0，cudnn 6.0，tf 1.4.0，driver 396.54  
注意conda install 的 cudatoolkit和cudnn不能取代本机安装的cuda和cudnn，也就是说本机要安cuda，cudnn，conda装tf时要装cudatoolkit，cudnn  

先装tensorflow再装-gpu 才能启用gpu 前者版本不能比后者高，libcudnn.so.x报错需要在conda里安装tf，tf-gpu。注意版本匹配

跑Octopus的实验时需要  
scipy>=1.0.0  
numpy>=1.16  
Keras>=2.2.0  
tensorflow_gpu>=1.11.0  
dirt  
否则会报：  
``` bash
(video2mesh) ty@ty-GE60-2PF:~/repos/octopus$ bash run_batch_demo.sh 
Using TensorFlow backend.
2019-05-29 15:18:24.784883: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-05-29 15:18:24.835566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-05-29 15:18:24.835835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 860M major: 5 minor: 0 memoryClockRate(GHz): 1.0195
pciBusID: 0000:01:00.0
totalMemory: 1.96GiB freeMemory: 1.08GiB
2019-05-29 15:18:24.835855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
Processing sample...
> Optimizing for pose...
  0%|          | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "infer_batch.py", line 87, in <module>
    main(args.weights, args.num, args.batch_file, args.opt_steps_pose, args.opt_steps_shape)
  File "infer_batch.py", line 46, in main
    model.opt_pose(segmentations, joints_2d, opt_steps=opt_pose_steps)
  File "/home/ty/repos/octopus/model/octopus.py", line 290, in opt_pose
    callbacks=[LambdaCallback(on_batch_end=lambda e, l: pbar.update(1))]
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/engine/training.py", line 1010, in fit
    self._make_train_function()
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/engine/training.py", line 509, in _make_train_function
    loss=self.total_loss)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/optimizers.py", line 475, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/optimizers.py", line 89, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2757, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py", line 555, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'smpl_body25face_layer_1_7/smpl_main/Svd' (op type: Svd)
```

显卡驱动还崩了 用ubuntu自带的怎么切驱动nvidia-smi都会报一行错  
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed 
and running.  
然后切不懂了卡在390 有这个问题https://askubuntu.com/questions/1035409/installing-nvidia-drivers-on-18-04  


2019.6.4 https://github.com/pmh47/dirt/issues/6 dirt inside cmakecache.txt add -DNDEBUG to CMAKE_CUDA_FLAGS:STRING

https://github.com/pmh47/dirt/issues/23  
tensorflow.python.framework.errors_impl.NotFoundError: /home/ty/repos/dirt/dirt/librasterise.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrESs

should not use conda install tensorflow & tensorflow-gpu, use pip install instead

nvidia driver keeps the newest one.

``` bash
(dirt) zhangtianyi@likun-ThinkStation:~/github/dirt$ python tests/square_test.py 
Traceback (most recent call last):
  File "tests/square_test.py", line 4, in <module>
    import dirt
  File "/home/zhangtianyi/github/dirt/dirt/__init__.py", line 2, in <module>
    from .rasterise_ops import rasterise, rasterise_batch, rasterise_deferred, rasterise_batch_deferred
  File "/home/zhangtianyi/github/dirt/dirt/rasterise_ops.py", line 6, in <module>
    _rasterise_module = tf.load_op_library(_lib_path + '/librasterise.so')
  File "/home/zhangtianyi/anaconda3/envs/dirt/lib/python2.7/site-packages/tensorflow/python/framework/load_library.py", line 61, in load_op_library
    lib_handle = py_tf.TF_LoadLibrary(library_filename)
tensorflow.python.framework.errors_impl.NotFoundError: /home/zhangtianyi/github/dirt/dirt/librasterise.so: undefined symbol: _ZN10tensorflow12OpDefBuilder4AttrESs

```
成功安装后test时出现上边的问题 -d_glibcxx_use_cxx11_abi=0改成1 gcc/g++版本从4.9换到5重装dirt就好了

总结：py2.7 tf1.13.2 cuda 显卡驱动装新的
1. setup.py里的dependence去掉用conda装
2. cmakelists.txt cmake_flag 加-d_glibcxx_use_cxx11_abi=1
3. change CMakeLists.txt line5 into  
`find_package(OpenGL REQUIRED COMPONENTS OpenGL EGL)`  
comment line9  
line53 into  
`target_link_libraries(rasterise OpenGL::OpenGL OpenGL::EGL ${Tensorflow_LINK_FLAGS})`  
As a hack, you can try directly linking the correct library: remove EGL from line 5 of CMakeLists (so FindOpenGL no longer searches for it), and at line 52, replace OpenGL::EGL by /usr/lib/nvidia-384/libEGL.so.1.1.0 
4. cmake ../csrc -D_OPENGL_LIB_PATH=/usr/lib/nvidia-390. 对应驱动版本
5. cmakecache.txt 加-dndebug
6. make
7. cd .. \ pip install -e .
8. tests


**笔记本会卡死！！！**

## Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation（3DV 2018）

### intro

已经有很多成功的工作，生成人体关键点，棒状表示模型（火柴人）（说的就是openpose）  
这里作者提出的是基于smpl的更具挑战性的任务：estimating the parameters of a detailed statistical human body model from a single image  

Traditional model-based approaches typically optimize an objective function that measures how well the model fits the image observations
传统的需要一个差不多初始化模型，然后把初值优化到最终结果（不需要3d训练数据——带3d动作标注的图片）  
CNN就是forward prediction models，就不需要initialization，但是需要3d姿态标注，不像2d标注好获得  

他们近期的工作通过把重建出的模型投影回2d空间更新损失函数，就可以使用2d标注了  
本文的**目的**：To analyze the importance of such components  
components: image--(CNN,3d notation trained)-->smpl model(hybird params)-->image--(reproject)-->2d notation for CNN training  
要形成闭环（loop）  
NBF = 一个包含统计身体模型的CNN  
两种监督模式：full 3d sup和weak 2d sup，bottom-up top-down的方法，使得NBF既不需要初始化模型也不需要3d标注的训练数据  
因为光照、衣服、杂乱的背景都不想要，专注于pose和shape，所以用处理后的image代替原始rgb image  
结论：
1. 12-body-part的分割就包含了足够的shape和pose信息
2. 这种处理后图像的方法比起用原图，效果有竞争力，更简单，训练数据利用率更高
3. 分割质量可以强有力预测fit质量

总结：
1. unites deep learning-based with traditional model-based methods
2. an in-depth analysis of the necessary components to achieve good performance in hybrid architectures and provide insights for its real-world applicability

### related work

Most model-based approaches fit a model to image evidence through complex non-linear optimization, requiring careful initialization to avoid poor local minima.  
用2d关键点是为了降低fitting复杂度  
lifting to 3D from 2D information alone is an ambiguous problem  

前人的工作有用rgb image的/image+2d keypoint的/2d keypoint+silhouette的  
NBF不需要初始化模型，用semantic segmentation做图片代理输入，原因有三：
1. 去除与3dpose无关的图像信息
2. 比keypoint和silhouette语义信息多
3. 允许分析精细程度（粒度）和placement对3d预测的重要程度

三个数据集UP-3D，HumanEva-I，Human3.6M  
{% pdf 19.6.25_weekly_report.pdf %}
三个数据集[UP-3D](http://files.is.tuebingen.mpg.de/classner/up/),
[HumanEva-I](http://humaneva.is.tue.mpg.de/datasets_human_1),
[Human3.6M](http://vision.imar.ro/human3.6m/description.php)  

up-3d有大量smpl形式的3d标注

其他数据集：


**HumanEva-I使用方法**：  
To be able to use HumanEva-I dataset you must do the following:  
  Sing up and agree with the license or Login if you already have an account.  
  Download the entire HumanEva-I dataset as either zip or tar archive depending on your system.   
  Download critical HumanEva-I update and update the OFS files.   
  Download the latest source code.    
  (optional) Download background statistics   
  (optional) Download the surface model for subject S4.   

装matlab, XVID codec, DXAVI toolbox, Camera Calibration Toolbox for Matlab  
给matlab指定了mingw作为c++编译器  

``` bash
Undefined function or variable 'dxAviOpenMex'.

Error in dxAviOpen (line 3)
	[hdl, t] = dxAviOpenMex(fname);

Error in testDxAvi (line 4)
[avi_hdl, avi_inf] = dxAviOpen([pathname, filename]);
```

运行mex_cmd出现

``` bash
F:\datasets\HumanEva-I\Release_Code_v1_1_beta\TOOLBOX_dxAvi\dxAviHelper.h:9:21: fatal error: atlbase.h: No such file or directory
 #include <atlbase.h>
```

应该是没有这个库的原因，有说是visual studio的库，打算装个vs2019 ATL库试试  
matlab不支持2019 mex -setup -v可以看到指搜索到vs2017  
所以装了vs2015，atlbase就可以了  
``` bash
Building with 'Microsoft Visual C++ 2015'.
Error using mex
dxAviOpenMex.cpp
BaseClasses\ctlutil.h(278): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
g:\grads\3dreconstruction\humaneva-i\release_code_v1_1_beta\toolbox_dxavi\dxAviHelper.h(15): fatal error C1083: Cannot open
include file: 'qedit.h': No such file or directory
```

原因在这里[link](https://github.com/facebookresearch/VideoPose3D/blob/master/DATASETS.md)

github上找了win64编译好的.m脚本，解决。 

**todo** 怎么做validation  

# 实验

## 复原实验

1. Extreme 3D Face Reconstruction: Seeing Through Occlusions [Github](https://github.com/anhttran/extreme_3d_faces)
   1. 环境：linux docker镜像
   2. 依赖：
      * our Bump-CNN
      * our PyTorch CNN model
      * the Basel Face Model
      * 3DDFA Expression Model
      * 3DMM_model
      * dlib face prediction model
2. Learning to Reconstruct People in Clothing from a Single RGB Camera [Github](https://github.com/thmoa/octopus)
   1. 环境：linux tf
   2. 依赖：
      * [DIRT](https://github.com/pmh47/dirt)
      * [SMPL model](http://smplify.is.tue.mpg.de/)
      * *[pre-trained model weights](https://drive.google.com/open?id=1_CwZo4i48t1TxIlIuUX3JDo6K7QdYI5r)*
   3. 备注：图片预处理需要
      * PGN semantic segmentation：Linux/tensorflow [Code](https://github.com/Engineering-Course/CIHP_PGN)
      * OpenPose body_25 and face keypoint detection：Win [.exe](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
3. Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation [Github](https://github.com/mohomran/neural_body_fitting)
   1. 环境：win/linux tensorflow-gpu==1.6.0
   2. 依赖：
      * [SMPL model(跟上边的还有区别)](http://smpl.is.tue.mpg.de/downloads)
      * [segmentation model](http://transfer.d2.mpi-inf.mpg.de/mohomran/nbf/refinenet_up.tgz)
      * [fitting model](http://transfer.d2.mpi-inf.mpg.de/mohomran/nbf/demo_up.tgz)
   3. 备注：没training code

## 数据集

1. HumanEva-I
   1. 环境：win/linux matlab
   2. 依赖：几个toolbox其中dxavi用的github上编译好的.m
2. UP-3D
   1. 环境：
   2. 依赖：
3. Human3.6M
   1. 注册不通过（20190716）

## repos

| repo name           | description                                                                                               |
| :------------------ | :-------------------------------------------------------------------------------------------------------- |
| VideoPose3D         | 3D human pose estimation in video with temporal convolutions and semi-supervised training                 |
| smplify-x           | Expressive Body Capture: 3D Hands, Face, and Body from a Single Image                                     |
| neural_body_fitting | Neural Body Fitting code repository                                                                       |
| octopus             | Learning to Reconstruct People in Clothing from a Single RGB Camera                                       |
| videoavatars        | Video based reconstruction of 3D people models                                                            |
| extreme_3d_faces    | Extreme 3D Face Reconstruction: Seeing Through Occlusions                                                 |
| 3Dpose_ssl          | 3D Human Pose Machines with Self-supervised Learning                                                      |
| pose-hg-train       | Training and experimentation code used for "Stacked Hourglass Networks for Human Pose Estimation"         |
| PRNet               | Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network (ECCV 2018)         |
| vrn                 | Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression                |
| openpose            | OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation    |

# Code

vscode想在不同的conda环境下都有类型提示和跳转需要在vscode里切环境  
ctrl+shift+P --> python:select interpreter --> {your env}  
[官方文档](https://code.visualstudio.com/docs/python/environments)  

import tensorflow 没有报错也没有反应：tensorflow-gpu跟conda安装的opencv有冲突！！  
改用`pip install opencv-python`就解决了  

同文件夹下module import要加`.`  
``` python
import tensorflow as tf
from .batch_smpl import SMPL
from .joints import joints_body25, face_landmarks
from keras.engine.topology import Layer
```

git-lfs在fork的repo上使用会有问题 "can not upload new objects to public fork"

## python module

tqdm: process bar tool
greenlet/gevent: 协程工具

## octopus

> 流程：  
读文件（segmentation/pose） png和json文件  
K.set_session启动tfsession  
声明model（octopus），加载weights  
解析segm：io.py里有解析segmentation的方法  
解析pose  
优化pose  
优化shape  
生成模型（点和面的list）  
写入obj（write_mesh）  

> opt_pose:  
> 两组数据: data/supervision  
> opt_pose_model.fit():
>   *   

> opt_shape:  
> data/supervision  
> opt_shape_model.fit()

想尝试把dirt换了，用别的differentiable renderer

## tex2shape

decectron2（pytorch环境）先做uv图  
tex2shape出模型，因为显存不够影响了重建效果（用video2mesh的conda环境就可以（tensorflow+keras））  
目前的代码是否可以训练模型，hdf5文件怎么生成（keras的hdf5文件，就是tf的ckpt，model.save就完事了，现在主要问题是fit train data）

## hmr End-to-end Recovery of Human Shape and Pose

有train code，可他妈太妙了  
数据预处理步骤：
* 数据集lsp --> tfrecord
* 

## datasets

- [LSP](http://sam.johnson.io/research/lsp_dataset.zip) and [LSP extended](http://sam.johnson.io/research/lspet_dataset.zip)
- [COCO](http://cocodataset.org/#download) we used 2014 Train. You also need to
  install the [COCO API](https://github.com/cocodataset/cocoapi) for python.
- [MPII](http://human-pose.mpi-inf.mpg.de/#download)
- [MPI-INF-3DHP](http://gvv.mpi-inf.mpg.de/3dhp-dataset/)
- [Human3.6M](http://vision.imar.ro/human3.6m/description.php)
- [Download link to MoSh](https://drive.google.com/file/d/1b51RMzi_5DIHeYh2KNpgEs8LVaplZSRP/view?usp=sharing)

### 训练数据预处理

TFRecord:数据序列化成二进制的工具

## keras

`keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)`  
Wraps arbitrary expression as a *Layer* object.

keras.backend: At this time, Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend.

LambdaCallback()  

## 要解决的问题

1. 现有数据集的数据怎么处理到能用在smpl上 ！！（解决 hmr里解决了训练数据-->tfrecord的过程）
2. 确定量化指标 ！！（解决 hmr有evaluation）
3. 确定遮挡情况下的重建效果！！（hmr，tex2shape，octopus，360texture那个）

## 实验室/作者汇总

| 名称                | 文章                   | 链接                  |
| :------------------ | :-------------------- | :-------------------- |
| MPI | SMPL/Octopus/.. | https://virtualhumans.mpi-inf.mpg.de/ |
| UCB(Angjoo Kanazawa) | 预测人体动作/动物形体重建 | https://people.eecs.berkeley.edu/~kanazawa/ |
| 周晓巍(浙大) | .. | http://www.cad.zju.edu.cn/home/xzhou/ |

## 技术要点汇总

| 文章名称                | 完成任务               | 技术要点描述           |
| :--------------------- | :-------------------- | :-------------------- |
| end to end recovery of human shape and pose(HMR) | an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image 不知道速度怎么样，其他的有做到实时的了 | 不计算2d/3d joint position，使用了一种高效的mesh representation parameterized by shape and joint angles|
| Learning to Reconstruct People in Clothing from a Single RGB Camera(Octopus) | 视频1-8帧做人体重建，10秒完成（说是速度快，但是其他的有做到实时的了） | 速度快归功于两点：Tpose下完成特征融合；using both, bottom-up and top-down streams（？？不理解回头看看） |
| Tex2Shape: Detailed Full Human Body Geometry from a Single Image | 单图重建模型，用了detectron的densepose对图像预处理出IUV图，然后根据原图+IUV图出模型 | 前置条件detectron/densepose/smpl |
| Multi-Garment Net: Learning to Dress 3D People from Images |  ||
| Learning 3D Human Dynamics from Video | single image预测人体3D past and future motion | present a framework that can similarly learn a representation of 3D dynamics of humans from video via a simple but effective temporal encoding of image features |
| Predicting 3D Human Dynamics from Video | 跟上边都是UCB的Predicting Human Dynamics (PHD), a neural autoregressive model that takes a video sequence of a person as input to predict the future 3D human mesh motion | 
| LiveCap:Real-time Human Performance Capture from Monocular Video | the first real-time human performance capture approach that reconstructs dense, space-time coherent deforming geometry of entire humans in general everyday clothing from just a single RGB video | 应该是预处理阶段重建模型（需要花费时间），实时添加动作。重点解决两个非线性优化问题，提出两阶段（stage）解决思路 |
| Three-D Safari: Learning to Estimate Zebra Pose, Shape, and Texture from Images “In the Wild” | 不需要图像分割/关节点标注的动物模型重建 | SMAL |
| PVNet: Pixel-wise Voting Network for 6DoF Pose Estimation | 对象姿态估计旨在检测对象并估计其相对于规范框架的方向和平移 |  PVNet predicts unit vectors that represent directions from each pixel of the object towards the keypoints. These directions then vote for the keypoint locations based on RANSAC//vector-field presentation |

## 实验规划

> 1. hmr：End-to-end Recovery of Human Shape and Pose
> 2. octopus 有模型 有纹理贴图 用了Detailed Human Avatars from Monocular Video.的贴图方法
> 3. tex2shape 这个有衣服的细节 试试有遮挡的情况下重建效果怎么样
> 4. Learning 3D Human Dynamics from Video
> 5. Multi-Garment Net: Learning to Dress 3D People from Images
> 6. pvnet 遮挡截断情况下可以做6DoF Pose Estimation

## 周计划

2019.10.21
1. 现有数据集的数据怎么处理到能用在smpl上 ！！（hmr有dataset-->tfrecord的code）
2. 确定量化指标 ！！ （hmr：跟3d groundtruth 点对点算距离，数据集human3.6m -- 这东西不知道啥时候能下载）

2019.10.28
1. 处理输入图片准备test（图片加遮挡，截断，运动模糊）
2. hmr、octopus、tex2shape 进行test
3. test结果进行量化评估

2019.11.4
1. 复原结果汇总
2. 贴图怎么上
3. 量化指标

2019.11.11
1. 训练code跑起来
2. 量化指标

## 日报

### 2019.10.28  
hmr，tex2shape环境部署  
todo：处理输入图像，查看结果  

### 2019.10.29
hmr结果已出，tex2shape需要densepose预处理图片，需要看看densepose对于遮挡，截断，运动模糊的处理情况  
todo densepose结果查看  

### 2019.10.30
detectron2可以用了，但是2提供的densepose的visualization mode不全，没有IUV，导致作为tex2shape的输入会有问题。还需要继续想办法  
todo：hmr基本上没有细节，只有pose和大致shape，接下来要主要关注tex2shape在有遮挡的情况下细节重建的效果  
找两个带贴图repo试试，octopus/garment/360  
evaluation没有human3.6做不了，那边注册不通过没法下载  

### 2019.10.31  
摸鱼

### 2019.11.1
detectron2里的densepose没法出IUV的图，不太明白IUV这个图怎么用opencv出。只能在densepose结果图上做遮挡看看tex2shape的重建效果了  
完成hmr/tex2shape的遮挡测试，todo：octopus/还有smpl加贴图

### 2019.11.2/3
休

### 2019.11.4
dirt 有个undefined symbol 大概率是跟显卡驱动 cuda版本有关系 因为笔记本上就装上了  
dirt装不上garment也没法跑，得想办法用opendr代替dirt  
dirt装上后有个segmentfault 明天继续看
整理tex2shape/hmr/octopus的结果  
明天看看贴图怎么搞，octopus用了个方法，还有garment那个的

### 2019.11.5
octopus keras.base_layer会报个参数错误  
densepose(tex2shape)不知道怎么出IUV  
garment(上贴图的)用了MPI-IS的mesh组件 需要python3
TODO: humaneva, garment, Semantic Human Texture Stitching

### 2019.11.6
量化指标：the mean per-pixel error of 3d displacements maps  
中文叫位移贴图/与凹凸贴图（法线贴图属于凹凸图），高度图不同  
贴图挺顺利的，理论上所有smpl的模型都适用。贴图这边接下来要看怎么用自己的数据（从img-->pkl-->texture）
octopus还是不行 操他妈的(keras outputs不是layer类型的，不知道为什么)（11.6更新：因为当时smpl()改成了smpl.call()，还是要走基类的__call__()的不然不是Layer类型）  
Lambda表达式是核心问题 明天看

### 2019.11.7
Octopus解决了，Lambda表达式没问题，smpl那个继承了Layer的类在调用__call__()时调用了call()，后者参数数量与基类Layer的call()参数数量不一致，导致了问题  
hmr的训练需要groundtruth 3d，先放着吧  
看effective C++(4/5)

### 2019.11.8
数据集MPI_inf_3dhp/MPII/COCO 下载  
下载数据集coco/mpii/mpi_inf_3dhp  
学习dx12

### 2019.11.9/10
休

### 2019.11.11
coco/lsp/lsp_ext/mocap_neutrMosh/mpii/mpi_inf_3dhp --> tfrecord  
hmr train code在tf1.14上有问题 降到1.4试试（conda最低1.4） hmr官方用的1.3//客制的有pytorch0.4的
trainer.py的train()有问题 --> sess.run时间太长了 1.4得调cuda版本 还是用1.14

### 2019.11.12
三个新论文 看起来实验会好做一些 train code/dataset都有：
* PyTorch implementation of CloudWalk's recent work DenseBody https://arxiv.org/pdf/1903.10153.pdf [github](https://github.com/Lotayou/densebody_pytorch)
* Repository for the paper "Convolutional Mesh Regression for Single-Image Human Shape Reconstruction" [github](https://github.com/nkolot/GraphCMR)
* Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation (CVPR2019 Oral) [github](https://github.com/zhuhao-nju/hmd)

### 2019.11.13
看看十大排序七大查找算法（3-0）  
hmd demo没啥问题 看看train 需要upi的数据集44g 这周五再下  
evl的用了wild dataset 1.9g//RECON and SYN test

### 2019.11.14
排序/查找算法

### 2019.11.15
upi_s1h//human36m_washed//two test dataset for hmd(eval recon and syn sets//wild set)  

### 2019.11.16/17
休

### 2019.11.18
train without coco & human3.6m coco需要联网用json下文件，实验室电脑没有那么多网关流量，human3.6m没数据集  
train joint的时候dataloader的num有点问题 改成8035试试（worked）done  
train anchor done  
eval test doing  
跑实验的同时看一下红黑树/B树/B+树  

### 2019.11.19
eval完成  
看hmd论文 `Detailed Human Shape Estimation from a Single Image by Hierarchical Mesh Deformation`

### 2019.11.20
tex2shape的模型是有uv的 octopus/hmd都没有uv 所以没法贴图  
加贴图那个基于octopus，需要绕着人转圈拍照片，然后做分割  
eval_wild on self trained model（10hrs）  
明天看看遮挡情况下hmd的重建效果

## conda env

  * pytorch：detectron2/densepose
  * tf2: python2 + tf1.14  hmr, tex2shape, Semantic Human Texture Stitching
  * tf: python3 + tf1.14 + pytorch  human_dynamics
  * dirt：py2.7 + tf1.13 + dirt  octopus, garment
  * hmd(可以跟tf2合并)：py2.7 + pytorch1.0.1  hmd

## 目标（朱青邮件内容）

研究方向：**非理想条件下的单目RGB相机三维人体重建**  

领域现状：目前基于相机阵列以及单目RGBD相机的三维人体重建技术已经较为成熟，仅依靠单目RGB相机的三维人体重建工作具有广阔的发展前景并且具有挑战性。以MPI、UCB、浙大为首的一些实验室已经在该研究方向上已经取得了一些成果，但是输入图像质量都比较理想，非理想条件下的重建效果并不明确。  

我的工作：目前确定做非理想条件下的单目相机三维人体重建，提高重建精度包括模型细节、姿态、纹理贴图。非理想条件具体来说有以下情况：
1. 图像中人物受到遮挡（重点）
2. 图像中人物因高速移动产生的运动模糊
3. 图像中人物因环境光照产生的视觉偏差

工作计划：看现有方法在上述非理想条件下的重建效果（文章中没有提到的需要亲自跑实验验证）；设计改善方法，反复实验验证，得到实验数据；论文撰写。

# 思路

* 单图多人（人群）三维重建  
  可能需要解决的问题：  
  遮挡（周晓巍的PVNet解决了遮挡的问题，空间维度上的估计）  
  分割  
  大小/相对位置  
  ...   
* 跟游戏开发能关联的地方：  
  用引擎看效果  
  实用性  

* 从视频序列中选出作用显著的帧，设计量化评价方法  

* 从不同表达，面点云体素区别入手  

* 增加脸部细节（手部、脚步，观察几个论文的演示视频好像都没有动作细节，骨骼的问题应该是）呢？？结合3dmm（已经有结合的了19.10.10更新）  

* 考虑多模态，加入语义信息辅助重建（还得看nlp的东西，把特征映射到一个空间不知道能不能做）

* **快速移动/运动模糊**的视频/照片做重建（回到图像处理的问题上，不确定目前已有的方法在视频中任务快速移动情况下的重建效果）

* UCB预测人体动作（时间维度上的估计） 能怎么改进

* MPI做的实时 

* UCB把SMPL用到了动物（斑马）模型重建；不是smpl是smal

* 光照条件对重建质量的影响

UCB做了动物的模型重建，根据视频**预测**人体接下来的动作；MPI**实时**Video to Mesh  

>>> shape：更有细节/遮挡、截断(空间维度预测)/  
>>> pose：根据视频、单图预测pose（时间维度预测）/实时更新pose  
>>> texture：单视角贴图/多视角贴图

> **疑问**
> 6D pose estimation 和 smpl/smal重建出的pose有何异同？？是一个东西吗  
> In contrast to coordinate or
heatmap based representations, learning such a representa-
tion enforces the network to focus on local features of ob-
jects and spatial relations between object parts. As a result,
the location of an invisible part can be inferred from the vis-
ible parts. In addition, this vector-field representation is able
to represent object keypoints that are even outside the input
image. All these advantages make it an ideal representation
for occluded or truncated objects.

## 时间安排

> ICIP 1月31日  
> 1月开始写论文  
> 12月实验，开题  
> 开始编写代码，训练模型，评估实验数据
> 11月实验  
> 设计优化思路，实验步骤，预期的实验结果 11.18-11.29 两周
> 现有方法在非理想情况下的表现 10.21-11.15 四周
> 10月底规划好实验步骤，预计出的结果
> 10月18号确定要做的目标

# 信息总结

fusion
mulity domin
多元融合

显著性
摘要
帧对重建质量的贡献

王少帆 北工大计算机学院
dblp

# todo list

数据清洗 三个数据集UP-3D，HumanEva-I，Human3.6M  
清洗的目的？目标？要做成什么样？