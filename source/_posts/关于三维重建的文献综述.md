---
title: 关于三维重建的文献综述
date: 2019-01-04 07:58:17
tags: 
    - 3D model reconstruction
    - 文献综述
categories: 论文综述
---

# 目录

[arXiv检索](https://arxiv.org/)

1. 1812.10558 通过视频素材实现从2d到3d的面部重建来完成测谎
2. 1812.01742 单一视角的三维重建，使用对抗训练（非人
3. 1812.05583 基于学习的ICP（迭代最近点算法）重构场景（非人
4. 1812.07603 通过视频素材的面部模型学习
5. 1812.05806 自我监督的引导方法，单图片的三维人脸重建
6. 1812.02822 学习生成模型的隐藏区域（非人
7. 1901.00049 基于轮廓的衣着人物（全身

**A类**

* 通过直接体积cnn回归从单图重建大范围三维人脸（源码lua+py
* 使用图到图转换的无限制面部重建（源码lua

**老师推荐**

* 使用affinity field的实时多人二维姿态估计

# 综述

## 通用

**关于三维重建**  
单个图像进行三维重建的数据驱动方法：一是明确使用三维结构，二是使用其他信息推断三维结构  
2DImage-->encoder-->latent representation-->decoder-->3DObject  
不同方法区别在于对三维世界采取的**限制**：多视图一致性学习三维表示、利用关键点和轮廓注释、利用2.5D草图（法线，深度和轮廓）改善预测  

encoder-decoder的[含义](https://blog.csdn.net/chinabing/article/details/78763454)

**关于shape priors**
许多方法选择更好的捕捉多样的真实形状  
**non-deep方法**关注低维参数模型，使用CNN来学习2D渲染图像和3D形状的**共同嵌入空间**  
其他方法依赖**生成模型**去学习shape priors

## 博客链接

[3D人脸重建学习笔记CSDN](https://blog.csdn.net/yyyllla/article/details/84573393)  
[3D重建的学习笔记简书](https://www.jianshu.com/p/f33b3d440f7d)

## Learning Single-View 3D Reconstruction with Adversarial Training 1812.01742  

传统方法用多个角度的多张照片实现三维建模  
问题两个：一是需要大量的观察点；二是物体表面是*Lambertian*（非反射）albedos是非均匀的  
另一种三维重建的方式是利用物体外观和形状的知识从单视图二维图像生成（假设shape priors足够丰富）  
CAD库（computer-aided design）：<u>shapenet，pascal3d+，objectnet3d，pix3d</u>  

这些方法都从渲染的图像中回归三维形状：将二位图像转化成潜在表示的**编码器** 以及 重建三维表示的**解码器**  
为了学习shape priors深度学习算法需要大量的三维对象注释，自然图像中获取三维注释很有挑战，因此使用合成图像（三维模型渲染出的图像）  
CNN的<u>domain shift</u>问题，导致基于cnn的三维重建性能恶化  

这篇文章的方法：提高重建模型性能，为了实现获取三维物体标签，他们shape priors训练出的网络有个**重建损失值**，给这个值引入了两个限制  
一是受domain shift文献启示，强制让编码的二维特征不变，对应于他们所来自的domain。这样合成图像训练出的编码器在真实图像上表现更好  
二是将编码的二维特征限制在现实物体的多种形状之中，通过对抗训练定义这两个损失值  
总结：一个**模型**和**损失函数**，利用shape priors提高自然图像三维重建性能（两种方式使用对抗训练）  
reconstruction adversarial network(RAN)  
**只使用rgb图像信息**，和易于获取的自然图像。独立于编码器和解码器，并且可以使用到其中  
借鉴了domain confusion（作用是classification），为了让从合成图像里训练出来的模型在真实图像这边有更好的表现  

具体方法：todo

## 通过直接体积cnn回归从单图重建大范围三维人脸

目前三维人脸重建的方法多假定有多张面部图片可以使用，这使得重建面临方法上的挑战：在夸张的表情、不均匀光照上建立稠密对应关系  
这些方法需要复杂低效的管道构建模型，拟合模型。本文建议通过在由2D图像和3D面部模型或扫描组成的适当数据集上训练卷积神经网络
（CNN）来解决这些限制

## Extreme 3D Face Reconstruction: Seeing Through Occlusions 极端3D面部重建：遮挡透视（讲）

bumpingmapping概念的推动下，该文提出了一种分层方法。将全局形状与其中细节进行解耦。估计粗糙的3d面部形状为基础，然后将此基础与凹凸贴图表示的细节分开。
与本文相关的工作：
    reconstruction by example 这类方法用三维脸部形状去调整根据输入图片估计出的模型，降低了观看条件却损失了真实度与准确性  
    face shape from facial landmarks 这类方法稳定但是模型都差不多，没有细节，而且不清楚遮挡landmark的情况下表现会如何  
    SfS *Shape-From-shading* 根据光反射生成细节丰富的模型，但是受环境影响严重，需要满足其对环境的特殊要求。任何遮挡物都会生成到模型中  
    statistical representations 最著名的方法是3DMM，这篇文改进了这个方法直接根据图片强度信息用cnn回归3DMM的参数和面部细节  
    deep face shape estimation 深度网络一是直接用深度图重建，二是estimate 3D shapes with anemphasis on unconstrained photo 观察条件高度不变但是细节模糊  

**准备工作**  
矛盾：整体形状的高度正则化vs细节的弱正则化。解决方法：bump map representations which separate global shape from local details
    理解的正则化：使模型更有普适性，低正则化是让模型有更多细节、更有特点，反之是让模型更接近普适的规则（每个模型都有一只鼻子一张嘴两只眼睛）
给一张图片建立以下几个部分：基础形状——S，面部表情——E，6维度的自由视点——V。接下来是bump map捕捉中级特征（皱纹等非参数的），最后完成因遮挡丢失的细节。  
**添加细节**
基础形状使用3DMM，3DMM用了resnet的101层网络架构。表情部分由3DDFA提供，更新的有expnet。确定视点用了deep，facepostnet。  
中等程度细节：image to bump map，修复遮挡细节，基于软对称的模型完善。  
[LFW验证](http://vis-www.cs.umass.edu/lfw/)

PPT用：
目的：现有单图三维重建局限性很高，必须在正前方、距离近、无阻挡的视点，该文设计了一种用于在极端条件下提供细节丰富的面部三维重建模型的系统。极端条件包括，头部旋转以及遮挡
方法：简单讲步骤，关键的创新点，值得学习的点后边会细说。
总的来说：先创建面部整体的基础形状，与局部细节分开，在基础形状之上建立中等程度的面部特征。这样做可以保证极端条件下整体面部形状的稳定性。其他较新的方法往往用局部细节构建整体形状。
 构建基础形状s，构建面部表情e，构建视点v：*凹凸图可以分离整体形状和局部细节*
这仨东西分别是干什么用的：*基础形状使用3DMM，3DMM用了resnet的101层网络架构。表情部分由3DDFA提供，更新的有expnet。确定视点用了deep，facepostnet。*
image to bump map转换
凹凸图训练集：用深度编码-解码框架生成凹凸图
学习建立凹凸图：定义了自己的网络损失函数，可以在不牺牲高频细节的情况下抑制噪声
还原遮挡细节
给予范例的空洞填充方法
搜索参考集
混合细节
更复杂的修补
基于软对称的模型补全  

贡献：解决**对foundation的高度正则化** VS **对detail的低正则化** 两者的矛盾  

注：  
    bump map使用灰度值来提供高度信息，normal map使用xyz轴所对应的rgb信息
    [卷积与反卷积](https://github.com/vdumoulin/conv_arithmetic)

跑demo流程：
    NVIDIA-docker启动container，如果跑代码没有driver重新run一个，用readme里的run命令。
    之后会出现860m只支持cuda5.0的报错，需要[从源码编译pytorch](https://github.com/pytorch/pytorch#from-source)。首先docker里装anaconda
        wget https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2018.12-Linux-x86_64.sh
        应该不用在docker里装cuda和cudnn，直接安装pytorch的依赖然后安装pytorch应该就可以  
        在1080上不会出现上边的报错，完全按照README走就行。

[PPT](extreme_3d_face.pptx)

<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/extreme_3d_face.pptx' width=800 height=600 frameborder='1'></iframe>

## Learning to Estimate 3D Human Pose and Shape from a Single Color Image(讲) DOI:10.1109/CVPR.2018.00055

SCAPE:  shape  completion  and  animationof people
SMPL: A skinned multi-person linear model  
SMPL是一种参数化人体模型，与非参数化模型的区别在于，参数化的可以用函数映射的方式表达出来，或者说是可以解析的？非参数化则认为是通过实验记录到的模型，不存在解析表达式。  


Stacked Hourglass Networks
[资料一](https://blog.csdn.net/wangzi371312/article/details/81174452)
[资料二](https://blog.csdn.net/shenxiaolu1984/article/details/51428392)

[feature map](https://blog.csdn.net/dengheCSDN/article/details/77848246)
channel:
卷积核个数、特征图个数、通道个数关系

[PPT](Learning to Estimate 3D Human Pose and Shape from a Single Color Image.pptx)

<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/Learning to Estimate 3D Human Pose and Shape from a Single Color Image.pptx' width=800 height=600 frameborder='1'></iframe>

## O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis

还有adaptive o-cnn  
The main technical challenge of the O-CNN is to parallelize the O-CNN computations defined on the sparse octants so that they can be efficiently executed on the GPU  
We train this O-CNN model with 3D shape datasets and refine the O-CNN models with different back-ends for three shape analysis tasks, including object classification, shape retrieval, and shape segmentation.

## Pixel2Mesh（讲）

### code

编译tensorflow math_functions.hpp找不到。需要软链接这个玩意  
ln -s /usr/local/cuda/include/crt/math_functions.hpp /usr/local/cuda/include/math_functions.hpp  

关于eigen和cuda[资料](https://blog.csdn.net/O1_1O/article/details/80066236)  
makefile怎么写。  
hdf5 HDF（Hierarchical Data Format）是一种设计用于存储和组织大量数据的文件格式

__CUDACC_VER__ is no longer supported.的报错看来要更新[eigen3](https://blog.csdn.net/luojie140/article/details/80159227)才能解决
github上新版eigen考到anaconda的eigen和support里就可以成功编译cuda了

图卷积神经网络[资料](http://tkipf.github.io/graph-convolutional-networks/)
图卷积神经网络[材料](https://cloud.tencent.com/developer/news/330322)
**所有的卷积都是在探讨如何对局部数据按照某一个操作聚合，不同的操作方式就对应于不同的卷积。**学习卷积核的过程其实是学习局部聚合参数的过程

[PPT](pixel2mesh.pptx)
<iframe src='https://view.officeapps.live.com/op/view.aspx?src=https://taye310.github.io/2019/01/04/%E5%85%B3%E4%BA%8E%E4%B8%89%E7%BB%B4%E9%87%8D%E5%BB%BA%E7%9A%84%E6%96%87%E7%8C%AE%E7%BB%BC%E8%BF%B0/pixel2mesh.pptx' width=800 height=600 frameborder='1'></iframe>

## SMPL: A Skinned Multi-Person Linear Model(多篇基础，15年)

## Video Based Reconstruction of 3D People Models(讲，没用网络) DOI:10.1109/CVPR.2018.00875 (video2mesh)

第五页第一张图解决了人体非刚性的问题  
但是问题在于人必须转身后摆出相同的姿势 不允许姿势变化  
第二张图 可以随意动 不同时刻的深度图非张性的注册并融合到一个template上  
存在 phantom surface的问题 运动的快会四肢胳膊 两个头  
第三个 加了一个static的人体模型作为约束 运动速度可以更快  

第一个使用rgb相机 并且支持用户运动的重建方法  
主要思想 和visual hull相似 不同角度拍摄剪影进行重建  
visual hull的基本原理 几个角度拍摄 分割出前景得到silhouette  
然后从相机坐标到silhouette的每一个点可以做一条射线 形成的曲面成为silhouette cone  
用这些cone作为约束就可以重建出三维模型 可以类比为雕刻的过程  
去掉cone之外的部分 最终剩下的部分就是人体的形状  

标准的vh的问题是只能用于静态的物体 这篇的主要是讲怎么把vh用到动态的物体  
第八页 每帧姿势都不一样 要做的就是去除由于运动对cone造成的变化 称为unpose的过程  
用unpose的cone做三维重建  

使用的人体的三维表达：smpl 参数化模型 T是template的mean shape，Bs是体型变化造成的模型变化  
Bp是pose的变化带来的变化  
问题在于没有办法model衣服头发面部特征 基础上加了D offset用于表达smpl表达不了的信息  
  
四个步骤  
1 前景分割 获取silhouette cone， tracking获取人体模型的姿态  
2 利用pose信息做unpose操作 转到Tpose姿态下  
3 人体重建 包含衣服 头发 人脸的人体模型  
4 多视角图像生成人体贴图  

1 基于cnn的方法 2d drawn detection 图像分割的方法 前景分割生成silhouette  
优化第12页的能量函数 进行pose tracking //简单来说就是求最优的pose和shape的参数 和模型匹配到检测到的  
2d drawn detection和silhouette上//  
2 第一步得到的cone进行unpose 每一条射线进行unpose转到canonical pose下  
//两个数学表达式 射线的转换//  
任何一点vi 和 任何一条射线ri
3 利用unpose后的cone做三维重建 称为consensus shape，相比较SMPL/视频表现出的是可以对衣服进行重建  
过程可以通过优化一个能量公式实现  
Edata：模型上的点到unpose ray的距离  
三个正则项：lap保证局部光滑，body保证重建出的与smpl差距不大，symm保证左右对称  
4 有了几何信息后 生成appearance信息 生成texture map 第一步有每一帧的pose，精确的将模型覆盖到图像上  
通过//重投影获得贴图//  

用sfs（之前的文章有提到）可以提供更多细节，本文方法可以提高的地方
对**能量函数**的理解：构建能量函数就是我们用方程的最小值来描述我们想要达到的实际效果。[资料](https://blog.csdn.net/a6333230/article/details/80070586)

第一步最费时间 一帧一分钟 model和silhouette的匹配费时间  
穿裙子解决不了 改变不了smplmodel的拓扑结构 拉不过去  
基于cnn的分割已经接近于完美了 用的别人的方法 不是重点  
给纹理图上色：consensus shape 结合第一步的pose 精确匹配到每一帧的图像上 back projection  

{% pdf videobasedreconstructionof3dpeoplemodelsGAMES201850徐维鹏.pdf %}
[ppt](videobasedreconstructionof3dpeoplemodelsGAMES201850徐维鹏.pdf)

## Learning to Reconstruct People in Clothing from a Single RGB Camera（2019.4video2mesh延伸论文，同一实验室）

安装dirt遇到的问题：https://github.com/pmh47/dirt/issues/23
已经尝试过cuda10.1/10.0/9.2 cudnn都是对应版本，tensorflow单独测试成功
更改gcc/g++版本：https://blog.csdn.net/u012925946/article/details/84584830

最终安装dirt解决方法是：  
ubuntu 18.04，cuda 8.0，cudnn 6.0，tf 1.4.0，driver 396.54  
注意conda install 的 cudatoolkit和cudnn不能取代本机安装的cuda和cudnn，也就是说本机要安cuda，cudnn，conda装tf时要装cudatoolkit，cudnn  

先装tensorflow再装-gpu 才能启用gpu 前者版本不能比后者高，libcudnn.so.x报错需要在conda里安装tf，tf-gpu。注意版本匹配

跑Octopus的实验时需要  
scipy>=1.0.0  
numpy>=1.16  
Keras>=2.2.0  
tensorflow_gpu>=1.11.0  
dirt  
否则会报：  
``` bash
(video2mesh) ty@ty-GE60-2PF:~/repos/octopus$ bash run_batch_demo.sh 
Using TensorFlow backend.
2019-05-29 15:18:24.784883: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
2019-05-29 15:18:24.835566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:892] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-05-29 15:18:24.835835: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 860M major: 5 minor: 0 memoryClockRate(GHz): 1.0195
pciBusID: 0000:01:00.0
totalMemory: 1.96GiB freeMemory: 1.08GiB
2019-05-29 15:18:24.835855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 860M, pci bus id: 0000:01:00.0, compute capability: 5.0)
Processing sample...
> Optimizing for pose...
  0%|          | 0/10 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "infer_batch.py", line 87, in <module>
    main(args.weights, args.num, args.batch_file, args.opt_steps_pose, args.opt_steps_shape)
  File "infer_batch.py", line 46, in main
    model.opt_pose(segmentations, joints_2d, opt_steps=opt_pose_steps)
  File "/home/ty/repos/octopus/model/octopus.py", line 290, in opt_pose
    callbacks=[LambdaCallback(on_batch_end=lambda e, l: pbar.update(1))]
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/engine/training.py", line 1010, in fit
    self._make_train_function()
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/engine/training.py", line 509, in _make_train_function
    loss=self.total_loss)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/legacy/interfaces.py", line 91, in wrapper
    return func(*args, **kwargs)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/optimizers.py", line 475, in get_updates
    grads = self.get_gradients(loss, params)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/optimizers.py", line 89, in get_gradients
    grads = K.gradients(loss, params)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py", line 2757, in gradients
    return tf.gradients(loss, variables, colocate_gradients_with_ops=True)
  File "/home/ty/anaconda3/envs/video2mesh/lib/python2.7/site-packages/tensorflow/python/ops/gradients_impl.py", line 555, in gradients
    (op.name, op.type))
LookupError: No gradient defined for operation 'smpl_body25face_layer_1_7/smpl_main/Svd' (op type: Svd)
```

显卡驱动还崩了 用ubuntu自带的怎么切驱动nvidia-smi都会报一行错  
NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed 
and running.  
然后切不懂了卡在390 有这个问题https://askubuntu.com/questions/1035409/installing-nvidia-drivers-on-18-04


## Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation（3DV 2018）

### intro

已经有很多成功的工作，生成人体关键点，棒状表示模型（火柴人）（说的就是openpose）  
这里作者提出的是基于smpl的更具挑战性的任务：estimating the parameters of a detailed statistical human body model from a single image  

Traditional model-based approaches typically optimize an objective function that measures how well the model fits the image observations
传统的需要一个差不多初始化模型，然后把初值优化到最终结果（不需要3d训练数据——带3d动作标注的图片）  
CNN就是forward prediction models，就不需要initialization，但是需要3d姿态标注，不像2d标注好获得  

他们近期的工作通过把重建出的模型投影回2d空间更新损失函数，就可以使用2d标注了  
本文的**目的**：To analyze the importance of such components  
components: image--(CNN,3d notation trained)-->smpl model(hybird params)-->image--(reproject)-->2d notation for CNN training  
要形成闭环（loop）  
NBF = 一个包含统计身体模型的CNN  
两种监督模式：full 3d sup和weak 2d sup，bottom-up top-down的方法，使得NBF既不需要初始化模型也不需要3d标注的训练数据  
因为光照、衣服、杂乱的背景都不想要，专注于pose和shape，所以用处理后的image代替原始rgb image  
结论：
1. 12-body-part的分割就包含了足够的shape和pose信息
2. 这种处理后图像的方法比起用原图，效果有竞争力，更简单，训练数据利用率更高
3. 分割质量可以强有力预测fit质量

总结：
1. unites deep learning-based with traditional model-based methods
2. an in-depth analysis of the necessary components to achieve good performance in hybrid architectures and provide insights for its real-world applicability

### related work

Most model-based approaches fit a model to image evidence through complex non-linear optimization, requiring careful initialization to avoid poor local minima.  
用2d关键点是为了降低fitting复杂度  
lifting to 3D from 2D information alone is an ambiguous problem  

前人的工作有用rgb image的/image+2d keypoint的/2d keypoint+silhouette的  
NBF不需要初始化模型，用semantic segmentation做图片代理输入，原因有三：
1. 去除与3dpose无关的图像信息
2. 比keypoint和silhouette语义信息多
3. 允许分析精细程度（粒度）和placement对3d预测的重要程度

三个数据集UP-3D，HumanEva-I，Human3.6M  
{% pdf 19.6.25_weekly_report.pdf %}
三个数据集[UP-3D](http://files.is.tuebingen.mpg.de/classner/up/),
[HumanEva-I](http://humaneva.is.tue.mpg.de/datasets_human_1),
[Human3.6M](http://vision.imar.ro/human3.6m/description.php)  

up-3d有大量smpl形式的3d标注

其他数据集：


**HumanEva-I使用方法**：  
To be able to use HumanEva-I dataset you must do the following:  
  Sing up and agree with the license or Login if you already have an account.  
  Download the entire HumanEva-I dataset as either zip or tar archive depending on your system.   
  Download critical HumanEva-I update and update the OFS files.   
  Download the latest source code.    
  (optional) Download background statistics   
  (optional) Download the surface model for subject S4.   

装matlab, XVID codec, DXAVI toolbox, Camera Calibration Toolbox for Matlab  
给matlab指定了mingw作为c++编译器  

``` bash
Undefined function or variable 'dxAviOpenMex'.

Error in dxAviOpen (line 3)
	[hdl, t] = dxAviOpenMex(fname);

Error in testDxAvi (line 4)
[avi_hdl, avi_inf] = dxAviOpen([pathname, filename]);
```

运行mex_cmd出现

``` bash
F:\datasets\HumanEva-I\Release_Code_v1_1_beta\TOOLBOX_dxAvi\dxAviHelper.h:9:21: fatal error: atlbase.h: No such file or directory
 #include <atlbase.h>
```

应该是没有这个库的原因，有说是visual studio的库，打算装个vs2019 ATL库试试  
matlab不支持2019 mex -setup -v可以看到指搜索到vs2017  
所以装了vs2015，atlbase就可以了  
``` bash
Building with 'Microsoft Visual C++ 2015'.
Error using mex
dxAviOpenMex.cpp
BaseClasses\ctlutil.h(278): error C4430: missing type specifier - int assumed. Note: C++ does not support default-int
g:\grads\3dreconstruction\humaneva-i\release_code_v1_1_beta\toolbox_dxavi\dxAviHelper.h(15): fatal error C1083: Cannot open
include file: 'qedit.h': No such file or directory
```

原因在这里[link](https://github.com/facebookresearch/VideoPose3D/blob/master/DATASETS.md)

github上找了win64编译好的.m脚本，解决。 

**todo** 怎么做validation

# 实验

## 复原实验

1. Extreme 3D Face Reconstruction: Seeing Through Occlusions [Github](https://github.com/anhttran/extreme_3d_faces)
   1. 环境：linux docker镜像
   2. 依赖：
      * our Bump-CNN
      * our PyTorch CNN model
      * the Basel Face Model
      * 3DDFA Expression Model
      * 3DMM_model
      * dlib face prediction model
2. Learning to Reconstruct People in Clothing from a Single RGB Camera [Github](https://github.com/thmoa/octopus)
   1. 环境：linux tf
   2. 依赖：
      * [DIRT](https://github.com/pmh47/dirt)
      * [SMPL model](http://smplify.is.tue.mpg.de/)
      * *[pre-trained model weights](https://drive.google.com/open?id=1_CwZo4i48t1TxIlIuUX3JDo6K7QdYI5r)*
   3. 备注：图片预处理需要
      * PGN semantic segmentation：Linux/tensorflow [Code](https://github.com/Engineering-Course/CIHP_PGN)
      * OpenPose body_25 and face keypoint detection：Win [.exe](https://github.com/CMU-Perceptual-Computing-Lab/openpose)
3. Neural Body Fitting: Unifying Deep Learning and Model Based Human Pose and Shape Estimation [Github](https://github.com/mohomran/neural_body_fitting)
   1. 环境：win/linux tensorflow-gpu==1.6.0
   2. 依赖：
      * [SMPL model(跟上边的还有区别)](http://smpl.is.tue.mpg.de/downloads)
      * [segmentation model](http://transfer.d2.mpi-inf.mpg.de/mohomran/nbf/refinenet_up.tgz)
      * [fitting model](http://transfer.d2.mpi-inf.mpg.de/mohomran/nbf/demo_up.tgz)
   3. 备注：没training code

## 数据集

1. HumanEva-I
   1. 环境：win/linux matlab
   2. 依赖：几个toolbox其中dxavi用的github上编译好的.m
2. UP-3D
   1. 环境：
   2. 依赖：
3. Human3.6M
   1. 注册不通过（20190716）

## repos

| repo name           | description                                                                                               |
| :------------------ | :-------------------------------------------------------------------------------------------------------- |
| VideoPose3D         | 3D human pose estimation in video with temporal convolutions and semi-supervised training                 |
| smplify-x           | Expressive Body Capture: 3D Hands, Face, and Body from a Single Image                                     |
| neural_body_fitting | Neural Body Fitting code repository                                                                       |
| octopus             | Learning to Reconstruct People in Clothing from a Single RGB Camera                                       |
| videoavatars        | Video based reconstruction of 3D people models                                                            |
| extreme_3d_faces    | Extreme 3D Face Reconstruction: Seeing Through Occlusions                                                 |
| 3Dpose_ssl          | 3D Human Pose Machines with Self-supervised Learning                                                      |
| pose-hg-train       | Training and experimentation code used for "Stacked Hourglass Networks for Human Pose Estimation"         |
| PRNet               | Joint 3D Face Reconstruction and Dense Alignment with Position Map Regression Network (ECCV 2018)         |
| vrn                 | Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression                |
| openpose            | OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation    |

# Code

vscode想在不同的conda环境下都有类型提示和跳转需要在vscode里切环境  
ctrl+shift+P --> python:select interpreter --> {your env}  
[官方文档](https://code.visualstudio.com/docs/python/environments)

## python module

tqdm: process bar tool
greenlet/gevent: 协程工具

## octopus

> 流程：  
读文件（segmentation/pose） png和json文件  
K.set_session启动tfsession  
声明model（octopus），加载weights  
解析segm：io.py里有解析segmentation的方法  
解析pose  
优化pose  
优化shape  
生成模型（点和面的list）  
写入obj（write_mesh）  

> opt_pose:  
> 两组数据: data/supervision  
> opt_pose_model.fit():
>   *   

> opt_shape:  
> data/supervision  
> opt_shape_model.fit()

## keras

`keras.layers.Lambda(function, output_shape=None, mask=None, arguments=None)`  
Wraps arbitrary expression as a *Layer* object.

keras.backend: At this time, Keras has three backend implementations available: the TensorFlow backend, the Theano backend, and the CNTK backend.

LambdaCallback()  



# 思路

* 单图多人（人群）三维重建  
  可能需要解决的问题：  
  遮挡  
  分割  
  大小/相对位置  
  ...   
* 跟游戏开发能关联的地方：  
  用引擎看效果  
  实用性  

* 从视频序列中选出作用显著的帧，设计量化评价方法  

* 从不同表达，面点云体素区别入手  

* 增加脸部细节呢？？结合3dmm  


# 信息总结

fusion
mulity domin
多元融合

显著性
摘要
帧对重建质量的贡献

王少帆 北工大计算机学院
dblp

# todo list

数据清洗 三个数据集UP-3D，HumanEva-I，Human3.6M  
清洗的目的？目标？要做成什么样？