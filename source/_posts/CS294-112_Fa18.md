---
title: CS294-112 Fa18
date: 2018-12-21 14:19:30
tags: 
    - reinforcement learning
    - berkeley course
    - Sergey Levine
---

# Deep reinforcement learning  

## Week One

2018.12.6  

Introduction and Course Overview [Slide](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-1.pdf)  

* why now?
    reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization  
    experience replay : [material](https://datascience.stackexchange.com/questions/20535/what-is-experience-replay-and-what-are-its-benefits)  
    instructive training instances provided by human teachers  
    hierarchical learning  
    non-Markovian environments by having a memory of their past  

* what can DL&RL do well now?
    Acquire high degree of proficiency in domains governed by simple, known rules  
    Learn simple skills with raw sensory inputs, given enough experience  
    Learn from imitating enough humanprovided expert behavior  

* challenges
    Humans can learn incredibly quickly  
    Humans can reuse past knowledge  
    Not clear what the reward function should be  
    Not clear what the role of prediction should be  

## Week Two

2018.12.13  

Goals:

* understand definitions & notation  
* understand basic imitation learning algorithms
* understand their strengths & weeknesses

Sequential decision-making 顺序决策
经典监督学习模式(imitation learning)不能完成自动驾驶的原因：  

* 就算只犯一点点错误（预测结果与训练集数据不同）都会导致后续预测的偏差被逐渐放大  

markovian behavior  
DAgger.  

for markovian behavior we can use RNN, Recurrent Neural Networks  
for multimodal behavior  

1. output mixture of gaussians  
2. latent variable models  
3. autoregressive discretization  

cost function for imitation  
  
* [homework1](http://rail.eecs.berkeley.edu/deeprlcourse/static/homeworks/hw1.pdf) 因为mujoco有visual c++的依赖 还得安vs studio 准备装个linux虚拟机再做  
  
## Week Three  
  
2018.12.20  
tensorflow tutorial  
  
## Week Four  
  
2018.12.21  
  
作业1下周该做完了  
  
1. markov 决策过程的定义
2. rl问题的定义
3. rl算法剖析
4. rl算法的类型简介  

markov decision process是rl的基础
markov chain->mdp  
S:state A:action O:observation T:transition e:emission probability r:reward function->r:S*A->R  
objectives:  
    公式in slide  
state-action marginal-->finite  
**stationary distribution**-->infinite  
eigenvalue:特征值问题  
  
**algorithms**  
